#summary The status of the common transformation project.

= Temporary repositories =

https://github.com/otinn/llvm

https://github.com/otinn/clang

https://github.com/otinn/memory-safety-testing-utils

= Initial results =
|| bench          || num loads/stores || num optimized by opt1 || num optimized by opt2 || num optimized by opt3 || num optimized total || run-time checks before opt || run-time checks avoided ||
|| 401.bzip2      || 4157             || 167 (4%)              || 1334 (32%)            || 30 (1%)               || 1531 (37%)          || 10526244748                || 5514284227 (52%) ||
|| 429.mcf        || 559              || 38 (7%)               || 61 (11%)              || 2 (0%)                || 101 (18%)           || 1458123364                 || 237088293 (16%) ||
|| 433.milc       || 4550             || 666 (15%)             || 338 (7%)              || 0 (0%)                || 1004 (22%)          || 13522033249                || 3086602745 (23%) ||
|| 445.gobmk      || 23876            || 4357 (18%)            || 3155 (13%)            || 411 (2%)              || 7923 (33%)          || 19325263713                || 8205893064 (42%) ||
|| 456.hmmer      || 12391            || 1146 (9%)             || 1456 (12%)            || 27 (0%)               || 2629 (21%)          || 4904909455                 || 221111686 (5%) ||
|| 458.sjeng      || 5051             || 852 (17%)             || 399 (8%)              || 21 (0%)               || 1272 (25%)          || 3811134741                 || 595197870 (16%) ||
|| 462.libquantum || 1029             || 418 (41%)             || 51 (5%)               || 0 (0%)                || 469 (46%)           || 1616295534                 || 294378970 (18%) ||
|| 464.h264ref    || 32248            || 3098 (10%)            || 2960 (9%)             || 133 (0%)              || 6191 (19%)          || 28132170266                || 7325970456 (26%) ||
|| 470.lbm        || 365              || 35 (10%)              || 24 (7%)               || 0 (0%)                || 59 (16%)            || 1432961919                 || 289561808 (20%) ||
|| all            || 84226            || 10777 (13%)           || 9778 (12%)            || 624 (1%)              || 21179 (25%)         || 84729136989                || 25770089119 (30%) ||

== opt1 (optimize-fast-memory-checks) ==
This pass removes fast load/store checks (ones where the memory object is known and valid at the time of the check) if it can prove that the access is entirely inside the memory object.

{{{
; Code from 456.hmmer/src/mathsupport.c: ILogsum and init_ilogsum (inlined)
; This file shows examples of optimize-fast-memory-checks being applied.

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@ILogsum.firsttime.b = internal unnamed_addr global i1 false
@ilogsum_lookup = internal unnamed_addr global [20000 x i32] zeroinitializer, align 16

declare void @__fastloadcheck(i8*, i64, i8*, i64)

declare void @__faststorecheck(i8*, i64, i8*, i64)

declare double @exp(double) nounwind

declare double @log(double) nounwind

define i32 @ILogsum(i32 %p1, i32 %p2) nounwind uwtable {
entry:


; OPT1 first instance: this check will be removed by optimize-fast-memory-checks because it is obviously in bounds.
; In the C code this access corresponds to the load from "static int firsttime".
  call void @__fastloadcheck(i8* bitcast (i1* @ILogsum.firsttime.b to i8*), i64 1, i8* bitcast (i1* @ILogsum.firsttime.b to i8*), i64 1) nounwind
  %.b = load i1* @ILogsum.firsttime.b, align 1
  br i1 %.b, label %if.end, label %for.body.i

for.body.i:                                       ; preds = %entry, %for.body.i
  %indvars.iv.i = phi i64 [ %indvars.iv.next.i, %for.body.i ], [ 0, %entry ]
  %0 = sub nsw i64 0, %indvars.iv.i
  %1 = trunc i64 %0 to i32
  %conv.i = sitofp i32 %1 to float
  %conv1.i = fpext float %conv.i to double
  %mul.i = fmul double %conv1.i, 0x3FE62E42FEAD449C
  %div.i = fdiv double %mul.i, 1.000000e+03
  %call.i = tail call double @exp(double %div.i) nounwind
  %add.i = fadd double %call.i, 1.000000e+00
  %call2.i = tail call double @log(double %add.i) nounwind
  %mul3.i = fmul double %call2.i, 0x40968AC7B890D5A6
  %conv4.i = fptosi double %mul3.i to i32
  %arrayidx.i = getelementptr inbounds [20000 x i32]* @ilogsum_lookup, i64 0, i64 %indvars.iv.i
  %2 = bitcast i32* %arrayidx.i to i8*


; OPT1 second instance: this check will be removed by optimize-fast-memory-checks because each of the accesses is in the global variable ilogsum_lookup.
; In the C code this corresponds to the stores to ilogsum_lookup in the init_ilogsum function (inlined into ILogsum).
  call void @__faststorecheck(i8* %2, i64 4, i8* bitcast ([20000 x i32]* @ilogsum_lookup to i8*), i64 80000) nounwind
  store i32 %conv4.i, i32* %arrayidx.i, align 4
  %indvars.iv.next.i = add i64 %indvars.iv.i, 1
  %lftr.wideiv = trunc i64 %indvars.iv.next.i to i32
  %exitcond = icmp eq i32 %lftr.wideiv, 20000
  br i1 %exitcond, label %init_ilogsum.exit, label %for.body.i

init_ilogsum.exit:                                ; preds = %for.body.i


; OPT1 third instance: this is similar to the first instance but on the first run path.
  call void @__faststorecheck(i8* bitcast (i1* @ILogsum.firsttime.b to i8*), i64 1, i8* bitcast (i1* @ILogsum.firsttime.b to i8*), i64 1) nounwind
  store i1 true, i1* @ILogsum.firsttime.b, align 1
  br label %if.end

if.end:                                           ; preds = %entry, %init_ilogsum.exit
  %sub = sub nsw i32 %p1, %p2
  %cmp = icmp sgt i32 %sub, 19999
  br i1 %cmp, label %return, label %if.else

if.else:                                          ; preds = %if.end
  %cmp2 = icmp slt i32 %sub, -19999
  br i1 %cmp2, label %return, label %if.else4

if.else4:                                         ; preds = %if.else
  %cmp5 = icmp sgt i32 %sub, 0
  br i1 %cmp5, label %if.then6, label %if.else7

if.then6:                                         ; preds = %if.else4
  %idxprom = sext i32 %sub to i64
  %arrayidx = getelementptr inbounds [20000 x i32]* @ilogsum_lookup, i64 0, i64 %idxprom
  %3 = bitcast i32* %arrayidx to i8*


; This check will not be optimized away by any of the current load/store check optimizations.
; In the C code this corresponds to the load from ilogsum_lookup[diff].
  call void @__fastloadcheck(i8* %3, i64 4, i8* bitcast ([20000 x i32]* @ilogsum_lookup to i8*), i64 80000) nounwind
  %4 = load i32* %arrayidx, align 4
  %add = add nsw i32 %4, %p1
  br label %return

if.else7:                                         ; preds = %if.else4
  %sub8 = sub nsw i32 0, %sub
  %idxprom9 = sext i32 %sub8 to i64
  %arrayidx10 = getelementptr inbounds [20000 x i32]* @ilogsum_lookup, i64 0, i64 %idxprom9
  %5 = bitcast i32* %arrayidx10 to i8*


; This check will not be optimized away by any of the current load/store check optimizations.
; In the C code this corresponds to the load from ilogsum_lookup[-diff].
  call void @__fastloadcheck(i8* %5, i64 4, i8* bitcast ([20000 x i32]* @ilogsum_lookup to i8*), i64 80000) nounwind
  %6 = load i32* %arrayidx10, align 4
  %add11 = add nsw i32 %6, %p2
  br label %return

return:                                           ; preds = %if.else, %if.end, %if.else7, %if.then6
  %retval.0 = phi i32 [ %add, %if.then6 ], [ %add11, %if.else7 ], [ %p1, %if.end ], [ %p2, %if.else ]
  ret i32 %retval.0
}
}}}

== opt2 (optimize-identical-ls-checks) ==
This pass removes identical load/store checks that appear in the same basic block.
It works by iterating over the calls in a basic block.
If any call is encountered then one of three things happens:
1) It is a load/store check with a previously unseen access pointer and size pair. The pair is added to the known accesses cache.
2) It is a load/store check with a previously seen access pointer and size pair. The check is removed.
3) It is another call that is not in a whitelist of functions known to not deallocate memory. The known access cache is cleared.

{{{
; Code from 401.bzip2/src/spec.c: ran
; This file shows an example of optimize-identical-ls-checks being applied.

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@seedi = common global i64 0, align 8

declare void @__loadcheck(i8*, i64)

declare void @__storecheck(i8*, i64)

define double @ran() nounwind uwtable {
entry:


; OPT2: this check is identical to the store check except that this is a load check and the other is a store check.
  call void @__loadcheck(i8* bitcast (i64* @seedi to i8*), i64 8) nounwind
  %0 = load i64* @seedi, align 8
  %div = sdiv i64 %0, 127773
  %rem = srem i64 %0, 127773
  %mul = mul nsw i64 %rem, 16807
  %1 = mul i64 %div, -2836
  %sub = add i64 %mul, %1
  %cmp = icmp sgt i64 %sub, 0
  %add = add nsw i64 %sub, 2147483647
  %storemerge = select i1 %cmp, i64 %sub, i64 %add


; OPT2: this check will be removed by optimize-identical-ls-checks because it can fail only if the load check does.
  call void @__storecheck(i8* bitcast (i64* @seedi to i8*), i64 8) nounwind
  store i64 %storemerge, i64* @seedi, align 8
  %conv = sitofp i64 %storemerge to float
  %div2 = fmul float %conv, 0x3E00000000000000
  %conv3 = fpext float %div2 to double
  ret double %conv3
}
}}}

== opt3 (optimize-implied-fast-ls-checks) ==
This pass removes fast load/store checks that are implied by other fast load/store checks.
It works by traversing a dominator tree to find out which checks must always happen before other checks.
It removes fast load/store checks that are dominated by another fast load/store check with the same access offset, access size, and object size triple.

{{{
; Code from 429.mcf/src/pbeampp.c: sort_basket
; This file shows examples of optimize-implied-fast-ls-checks being applied.
; 
; The dominator tree below may help understand why the optimization applies.
; [4] %while.cond {3,20} (first implication)
;   [5] %while.cond3 {4,19} (second implication)
;     [6] %while.end8 {5,18}
;       [7] %if.then {6,7} (both removed checks)

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.basket = type { %struct.arc*, i64, i64 }
%struct.arc = type { i64, %struct.node*, %struct.node*, i32, %struct.arc*, %struct.arc*, i64, i64 }
%struct.node = type { i64, i32, %struct.node*, %struct.node*, %struct.node*, %struct.node*, %struct.arc*, %struct.arc*, %struct.arc*, %struct.arc*, i64, i64, i32, i32 }

@perm = internal unnamed_addr global [351 x %struct.basket*] zeroinitializer, align 16

declare void @__loadcheck(i8*, i64)

declare void @__fastloadcheck(i8*, i64, i8*, i64)

declare void @__faststorecheck(i8*, i64, i8*, i64)

define void @sort_basket(i64 %min, i64 %max) nounwind uwtable {
entry:
  br label %tailrecurse

tailrecurse:                                      ; preds = %if.end22, %entry
  %min.tr = phi i64 [ %min, %entry ], [ %l.0.inc, %if.end22 ]
  %add = add nsw i64 %min.tr, %max
  %div = sdiv i64 %add, 2
  %arrayidx = getelementptr inbounds [351 x %struct.basket*]* @perm, i64 0, i64 %div
  %0 = bitcast %struct.basket** %arrayidx to i8*
  call void @__fastloadcheck(i8* %0, i64 8, i8* bitcast ([351 x %struct.basket*]* @perm to i8*), i64 2808) nounwind
  %1 = load %struct.basket** %arrayidx, align 8
  %abs_cost = getelementptr inbounds %struct.basket* %1, i64 0, i32 2
  %2 = bitcast i64* %abs_cost to i8*
  call void @__loadcheck(i8* %2, i64 8) nounwind
  %3 = load i64* %abs_cost, align 8
  br label %while.cond.outer

while.cond.outer:                                 ; preds = %if.end, %tailrecurse
  %l.0.ph = phi i64 [ %min.tr, %tailrecurse ], [ %l.0.inc, %if.end ]
  %r.0.ph = phi i64 [ %max, %tailrecurse ], [ %r.1.dec, %if.end ]
  br label %while.cond


; This is the condition block in the first while loop of the function.
while.cond:                                       ; preds = %while.cond, %while.cond.outer
  %l.0 = phi i64 [ %inc, %while.cond ], [ %l.0.ph, %while.cond.outer ]
  %arrayidx1 = getelementptr inbounds [351 x %struct.basket*]* @perm, i64 0, i64 %l.0
  %4 = bitcast %struct.basket** %arrayidx1 to i8*


; OPT3 instance 1: this check is identical to the first removed check except that this is a fast load check and the other is a fast store check.
  call void @__fastloadcheck(i8* %4, i64 8, i8* bitcast ([351 x %struct.basket*]* @perm to i8*), i64 2808) nounwind
  %5 = load %struct.basket** %arrayidx1, align 8
  %abs_cost2 = getelementptr inbounds %struct.basket* %5, i64 0, i32 2
  %6 = bitcast i64* %abs_cost2 to i8*
  call void @__loadcheck(i8* %6, i64 8) nounwind
  %7 = load i64* %abs_cost2, align 8
  %cmp = icmp sgt i64 %7, %3
  %inc = add nsw i64 %l.0, 1
  br i1 %cmp, label %while.cond, label %while.cond3

; This is the condition block of the second while loop in the function.
while.cond3:                                      ; preds = %while.cond, %while.cond3
  %r.1 = phi i64 [ %dec, %while.cond3 ], [ %r.0.ph, %while.cond ]
  %arrayidx4 = getelementptr inbounds [351 x %struct.basket*]* @perm, i64 0, i64 %r.1
  %8 = bitcast %struct.basket** %arrayidx4 to i8*


; OPT3 instance 2: this check is identical to the second removed check except that this is a fast load check and the other is a fast store check.
  call void @__fastloadcheck(i8* %8, i64 8, i8* bitcast ([351 x %struct.basket*]* @perm to i8*), i64 2808) nounwind
  %9 = load %struct.basket** %arrayidx4, align 8
  %abs_cost5 = getelementptr inbounds %struct.basket* %9, i64 0, i32 2
  %10 = bitcast i64* %abs_cost5 to i8*
  call void @__loadcheck(i8* %10, i64 8) nounwind
  %11 = load i64* %abs_cost5, align 8
  %cmp6 = icmp sgt i64 %3, %11
  %dec = add nsw i64 %r.1, -1
  br i1 %cmp6, label %while.cond3, label %while.end8

while.end8:                                       ; preds = %while.cond3
  %cmp9 = icmp slt i64 %l.0, %r.1
  br i1 %cmp9, label %if.then, label %if.end


; This is the first if-then block of the function.
if.then:                                          ; preds = %while.end8


; OPT3 instance 1: this check is removed by optimize-implied-fast-ls-checks because it is implied by the fast load check in while.cond.
  call void @__faststorecheck(i8* %4, i64 8, i8* bitcast ([351 x %struct.basket*]* @perm to i8*), i64 2808) nounwind
  store %struct.basket* %9, %struct.basket** %arrayidx1, align 8


; OPT3 instance 2: this check is removed by optimize-implied-fast-ls-checks because it is implied by the fast load check in while.cond3.
  call void @__faststorecheck(i8* %8, i64 8, i8* bitcast ([351 x %struct.basket*]* @perm to i8*), i64 2808) nounwind
  store %struct.basket* %5, %struct.basket** %arrayidx4, align 8
  br label %if.end

if.end:                                           ; preds = %if.then, %while.end8
  %cmp14 = icmp sgt i64 %l.0, %r.1
  %l.0.inc = select i1 %cmp14, i64 %l.0, i64 %inc
  %r.1.dec = select i1 %cmp14, i64 %r.1, i64 %dec
  %cmp19 = icmp sgt i64 %l.0.inc, %r.1.dec
  br i1 %cmp19, label %do.end, label %while.cond.outer

do.end:                                           ; preds = %if.end
  %cmp20 = icmp sgt i64 %r.1.dec, %min.tr
  br i1 %cmp20, label %if.then21, label %if.end22

if.then21:                                        ; preds = %do.end
  tail call void @sort_basket(i64 %min.tr, i64 %r.1.dec)
  br label %if.end22

if.end22:                                         ; preds = %if.then21, %do.end
  %cmp23 = icmp slt i64 %l.0.inc, %max
  %cmp24 = icmp slt i64 %l.0.inc, 51
  %or.cond = and i1 %cmp23, %cmp24
  br i1 %or.cond, label %tailrecurse, label %if.end26

if.end26:                                         ; preds = %if.end22
  ret void
}
}}}


= Interface =
{{{
// added before every load
void __loadcheck(void *ptr, size_t size);
void __fastloadcheck(void *ptr, size_t size, void *obj, size_t obj_size);

// added before every store
void __storecheck(void *ptr, size_t size);
void __faststorecheck(void *ptr, size_t size, void *obj, size_t obj_size);

// called when a fast load/store check has been inlined and fails
void __fail_fastloadcheck(void *ptr, size_t size, void *obj, size_t obj_size);
void __fail_faststorecheck(void *ptr, size_t size, void *obj, size_t obj_size);

// added after every gep; retuns a checked dest pointer (possibly a trap address)
void* __gepcheck(void *src, void *dest);
void* __fastgepcheck(void *src, void *dest, void *obj, size_t obj_size);

// added before every call through a function pointer
void __funccheck(void *func);

// added before every free
void __freecheck(void *ptr);

// added after every alloca instruction (or the whole stack if they are in a super-alloca)
void __pool_register_stack(void *obj, size_t obj_size);
// added before every return from a function for every existing __pool_register_stack
void __pool_unregister_stack(void *obj);

// added for every global in a special function, registers all globals and their sizes
void __pool_register_global(void *obj, size_t obj_size);

// added after every allocation on the heap (for example malloc); excludes realloc-like allocation
void __pool_register_heap(void *obj, size_t obj_size);
// added after the deallocation of every a heap object
void __pool_unregister_heap(void *obj);

// added after every realloc-like allocation on the heap
void __pool_reregister_heap(void *old_obj, void *new_obj, size_t new_size);
}}}

= Independently unnecessary load/store checks partitions = 
{{{
1. index problem
1.1. none
1.2. access starts before the object
1.3. access ends after the object

2. allocation problem
2.1. none
2.2. possibly freed
2.3. unknown allocation

3. index type
3.1. constant
3.2. simple expression (which limitations apply?)
3.3. anything else

4. object size type
4.1. constant
4.2. simple expression (which limitations apply?)
4.3. anything else

5. number of objects
5.1. single object
5.2. multiple objects

6. number of object types
6.1. one type
6.2. two or more types

7. number of object sizes
7.1. one size
7.2. two or more sizes

8. reason for multiple objects
8.1. N/A: one object
8.2. select
8.3. phi
8.4. mixed

9. loop structure
9.1. none
9.2. single loop
9.3. multiple loops
}}}

= Partitions for load/store checks that may be implied by other such checks =
{{{
1. index problem 
1.1. none
1.2. off in the same direction
1.3. off in different directions

2. allocation problem
2.1. none
2.2. first check on a possibly freed object
2.3. last check on a possibly freed object
2.4. both checks on a possibly freed object

3. boundary expression type
3.1. constant
3.2. simple expression (which limitations apply?)
3.3. unknown
3.4. anything else

4. number of memory object sets
4.1. one
4.2. two

5. CFG structure
5.1. same basic block
5.2. single connecting path
5.3. multiple connecting paths

6. loop structure
6.1. none
6.2. checks in the same loop
6.3. checks in different loops / one not in a loop
}}}

= Partitions for load/store/gep checks that can be converted to their fast versions =
{{{
1. allocation type
1.1. regular stack
1.2. scoped alloca
1.3. global
1.4. heap
1.5. mixed
1.6. unknown

2. allocation problem
2.1. none
2.2. possibly freed
2.3. possible to override global variable's size

3. object size type
3.1. constant
3.2. variable
3.3. unknown

4. number of objects
4.1. single object
4.2. multiple objects

5. number of object types
5.1. one type
5.2. two or more types

6. number of object sizes
6.1. one size
6.2. two or more sizes

7. reason for multiple objects
7.1. N/A: one object
7.2. select
7.3. phi
7.4. mixed

8. loop structure
8.1. none
8.2. single loop
8.3. multiple loops

9. CFG structure
9.1. same basic block
9.2. single connecting path
9.3. multiple connecting paths
}}}